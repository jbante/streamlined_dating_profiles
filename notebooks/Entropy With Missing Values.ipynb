{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"import entropy as ent\\nimport functools as fn\\nfrom importlib import reload\\nimport itertools as itr\\nimport numba as nb\\nimport numpy as np\\nimport numpy.linalg as la\\nimport pandas as pd\\nimport parametersearch as ps\\nimport pyitlib.discrete_random_variable as drv\\nimport scipy.spatial.distance as dist\\nimport scipy.stats as st\\nimport score as sc\\nimport utility as utl\\n\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"import entropy as ent\\nimport functools as fn\\nfrom importlib import reload\\nimport itertools as itr\\nimport numba as nb\\nimport numpy as np\\nimport numpy.linalg as la\\nimport pandas as pd\\nimport parametersearch as ps\\nimport pyitlib.discrete_random_variable as drv\\nimport scipy.spatial.distance as dist\\nimport scipy.stats as st\\nimport score as sc\\nimport utility as utl\\n\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import functools as fn\n",
    "import itertools as itr\n",
    "from importlib import reload\n",
    "\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import pandas as pd\n",
    "import pyitlib.discrete_random_variable as drv\n",
    "import scipy.spatial.distance as dist\n",
    "import scipy.stats as st\n",
    "\n",
    "import entropy as ent\n",
    "import parametersearch as ps\n",
    "import score as sc\n",
    "import utility as utl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# load user profile data\\ndf = pd.read_feather(\\\"../processed_data/profiles.feather\\\")\\nrow_missing_count = df[\\\"missing_count\\\"]\\ndf = df.drop(columns=\\\"missing_count\\\")\\n\\n# recode missing values as np.nan\\nMISSING_IND = np.nan\\ndf = df.replace(to_replace=-1, value=MISSING_IND)\\n\\nmissing_mask = fn.partial(ent._missing_mask, missing_ind=MISSING_IND)\\npdist = fn.partial(utl.pdist, n_jobs=-1)\";\n",
       "                var nbb_formatted_code = \"# load user profile data\\ndf = pd.read_feather(\\\"../processed_data/profiles.feather\\\")\\nrow_missing_count = df[\\\"missing_count\\\"]\\ndf = df.drop(columns=\\\"missing_count\\\")\\n\\n# recode missing values as np.nan\\nMISSING_IND = np.nan\\ndf = df.replace(to_replace=-1, value=MISSING_IND)\\n\\nmissing_mask = fn.partial(ent._missing_mask, missing_ind=MISSING_IND)\\npdist = fn.partial(utl.pdist, n_jobs=-1)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load user profile data\n",
    "df = pd.read_feather('../processed_data/profiles.feather')\n",
    "row_missing_count = df['missing_count']\n",
    "df = df.drop(columns='missing_count')\n",
    "\n",
    "# recode missing values as np.nan\n",
    "MISSING_IND = np.nan\n",
    "df = df.replace(to_replace=-1, value=MISSING_IND)\n",
    "\n",
    "missing_mask = fn.partial(ent._missing_mask, missing_ind=MISSING_IND)\n",
    "pdist = fn.partial(utl.pdist, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2603.000000\n",
       "mean        0.760982\n",
       "std         0.192352\n",
       "min         0.000000\n",
       "25%         0.666145\n",
       "50%         0.807740\n",
       "75%         0.905633\n",
       "max         0.998523\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# check for missing data\\nmissing_rate = df.isna().mean()\\nmissing_rate.describe()\";\n",
       "                var nbb_formatted_code = \"# check for missing data\\nmissing_rate = df.isna().mean()\\nmissing_rate.describe()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check for missing data\n",
    "missing_rate = df.isna().mean()\n",
    "missing_rate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2382 columns (92%) are empty in at least half the rows.\n",
      "76% of all cells are empty.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# check for columns with less than half missing values\\nat_least_half = missing_rate[missing_rate < 0.5]\\nn_less_than_half = len(missing_rate) - len(at_least_half)\\np_less_than_half = n_less_than_half / len(missing_rate)\\nprint(\\n    f\\\"{n_less_than_half} columns ({p_less_than_half:.0%}) are empty in at least half the rows.\\\"\\n)\\n\\ntotal_missing = missing_rate.mean()\\nprint(f\\\"{total_missing:.0%} of all cells are empty.\\\")\";\n",
       "                var nbb_formatted_code = \"# check for columns with less than half missing values\\nat_least_half = missing_rate[missing_rate < 0.5]\\nn_less_than_half = len(missing_rate) - len(at_least_half)\\np_less_than_half = n_less_than_half / len(missing_rate)\\nprint(\\n    f\\\"{n_less_than_half} columns ({p_less_than_half:.0%}) are empty in at least half the rows.\\\"\\n)\\n\\ntotal_missing = missing_rate.mean()\\nprint(f\\\"{total_missing:.0%} of all cells are empty.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check for columns with less than half missing values\n",
    "at_least_half = missing_rate[missing_rate < 0.5]\n",
    "n_less_than_half = len(missing_rate) - len(at_least_half)\n",
    "p_less_than_half = n_less_than_half / len(missing_rate)\n",
    "print(\n",
    "    f'{n_less_than_half} columns ({p_less_than_half:.0%}) are empty in at least half the rows.'\n",
    ")\n",
    "\n",
    "total_missing = missing_rate.mean()\n",
    "print(f'{total_missing:.0%} of all cells are empty.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is dominated by missing values: 77% of all cells are empty. 93% of variables are empty for at least half the profiles. Most users don't answer most questions. The values are missing not-at-random (MNAR):\n",
    "\n",
    "- Users each answer different numbers of questions.\n",
    "- OKCupid prioritizes some questions over others in the order they are presented to users.\n",
    "- Users can choose to not answer any particular question, or to hide their response.\n",
    "\n",
    "Treating missing values in each variable as a distinct informative class would likely grossly distort similarities between users. Imputation would be problematic with data MNAR and with the volume of missing data. It will be more appropriate to use methods that can tolerate missing values as an absence of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The existing popular functions for calculating entropy on data do not handle missing values well. Simply calculating entropy on the remaining data for a variable as if there were no missing values represents the variable as more informative than it actually is. Treating missing values equally as a distinct class falsely inflates the value of a variable even more. Considering that the entropy of a data set represents the expected length of a key that would be needed to uniquely identify each distinct item, non-missing values of a variable divide the data into smaller subsets that need smaller keys to index, and the difference in expected key length is the entropy of the variable. Missing values do not sub-divide the data—data with missing values require just as long a key to index with that variable as without it—and thus make the variable as a whole less informative. I created a separate entropy calculation that correctly takes this into account to use for feature selection. This particular calculation is only applicable to nominal data. More work is necessary to adapt the mathematics to numerical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entropy of a, which has no missing values, is 1.585 bits.\n",
      "The entropy of b, which is missing half its values, is 1.585 bits when calculated by ignoring missing values.\n",
      "The entropy of b is 1.792 bits when calculated by treating missing values as a distinct class.\n",
      "The entropy of b is 0.792 bits when calculated with missing values in mind.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"a = [0, 0, 1, 1, 2, 2]\\nprint(f\\\"The entropy of a, which has no missing values, is {drv.entropy(a):.3f} bits.\\\")\\n\\nb = np.array([0, 1, 2, -1, -1, -1])  # pyitlib read -1 as a missing value\\nprint(\\n    f\\\"The entropy of b, which is missing half its values, is {drv.entropy(b):.3f} bits when calculated by ignoring missing values.\\\"\\n)\\nprint(\\n    f\\\"The entropy of b is {drv.entropy(pd.Series(b)):.3f} bits when calculated by treating missing values as a distinct class.\\\"\\n)\\nb = np.where(b == -1, np.nan, b)  # code missing values as np.nan\\nprint(\\n    f\\\"The entropy of b is {ent.entropy(b):.3f} bits when calculated with missing values in mind.\\\"\\n)\";\n",
       "                var nbb_formatted_code = \"a = [0, 0, 1, 1, 2, 2]\\nprint(f\\\"The entropy of a, which has no missing values, is {drv.entropy(a):.3f} bits.\\\")\\n\\nb = np.array([0, 1, 2, -1, -1, -1])  # pyitlib read -1 as a missing value\\nprint(\\n    f\\\"The entropy of b, which is missing half its values, is {drv.entropy(b):.3f} bits when calculated by ignoring missing values.\\\"\\n)\\nprint(\\n    f\\\"The entropy of b is {drv.entropy(pd.Series(b)):.3f} bits when calculated by treating missing values as a distinct class.\\\"\\n)\\nb = np.where(b == -1, np.nan, b)  # code missing values as np.nan\\nprint(\\n    f\\\"The entropy of b is {ent.entropy(b):.3f} bits when calculated with missing values in mind.\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = [0, 0, 1, 1, 2, 2]\n",
    "print(\n",
    "    f'The entropy of a, which has no missing values, is {drv.entropy(a):.3f} bits.')\n",
    "\n",
    "b = np.array([0, 1, 2, -1, -1, -1])  # pyitlib read -1 as a missing value\n",
    "print(\n",
    "    f'The entropy of b, which is missing half its values, is {drv.entropy(b):.3f} bits when calculated by ignoring missing values.'\n",
    ")\n",
    "print(\n",
    "    f'The entropy of b is {drv.entropy(pd.Series(b)):.3f} bits when calculated by treating missing values as a distinct class.'\n",
    ")\n",
    "b = np.where(b == -1, np.nan, b)  # code missing values as np.nan\n",
    "print(\n",
    "    f'The entropy of b is {ent.entropy(b):.3f} bits when calculated with missing values in mind.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.86813234)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"drv.entropy(df[\\\"q2\\\"])\";\n",
       "                var nbb_formatted_code = \"drv.entropy(df[\\\"q2\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drv.entropy(df['q2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.315390137859471"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"ent.entropy(df[\\\"q2\\\"])\";\n",
       "                var nbb_formatted_code = \"ent.entropy(df[\\\"q2\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ent.entropy(df['q2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2603.000000\n",
       "mean        0.249387\n",
       "std         0.233377\n",
       "min         0.000255\n",
       "25%         0.087877\n",
       "50%         0.182209\n",
       "75%         0.337961\n",
       "max         2.922291\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"ent.entropy(df).describe()\";\n",
       "                var nbb_formatted_code = \"ent.entropy(df).describe()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ent.entropy(df).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2603.000000\n",
       "mean        0.171020\n",
       "std         0.139985\n",
       "min         0.000255\n",
       "25%         0.062566\n",
       "50%         0.137574\n",
       "75%         0.243618\n",
       "max         0.949562\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"ent.normalized_entropy(df).describe()\";\n",
       "                var nbb_formatted_code = \"ent.normalized_entropy(df).describe()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ent.normalized_entropy(df).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3967889524996573"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"cols = df.columns\\nent.info_variation(df[cols[0]], df[cols[1]])\";\n",
       "                var nbb_formatted_code = \"cols = df.columns\\nent.info_variation(df[cols[0]], df[cols[1]])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = df.columns\n",
    "ent.info_variation(df[cols[0]], df[cols[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.39678895, 0.85873146, 0.77348966, 0.80217984],\n",
       "       [1.39678895, 0.        , 1.36070029, 1.30574991, 1.33855463],\n",
       "       [0.85873146, 1.36070029, 0.        , 0.73885013, 0.76436172],\n",
       "       [0.77348966, 1.30574991, 0.73885013, 0.        , 0.48673748],\n",
       "       [0.80217984, 1.33855463, 0.76436172, 0.48673748, 0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"dist.squareform(pdist(df[cols[:5]].values.T, ent.info_variation))\";\n",
       "                var nbb_formatted_code = \"dist.squareform(pdist(df[cols[:5]].values.T, ent.info_variation))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dist.squareform(pdist(df[cols[:5]].values.T, ent.info_variation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"ent.mutual_info(df[cols[1]], df[cols[2]])\";\n",
       "                var nbb_formatted_code = \"ent.mutual_info(df[cols[1]], df[cols[2]])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ent.mutual_info(df[cols[1]], df[cols[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hamming distance is a popular information-theoretic measure of distance between points composed of categorical data. The SciPy implementation treats NaN values almost the same as any other value, with the exception that two NaNs in the same coordinate are treated as not equal. There is a certain logic to this, but it's inconsistent with the spirit of my revised entropy calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Normalized) Hamming distance treats NaN the same as any other value\n",
      "[[0.         0.66666667 0.66666667 1.         1.        ]\n",
      " [0.66666667 0.         0.33333333 0.66666667 1.        ]\n",
      " [0.66666667 0.33333333 0.         0.33333333 0.66666667]\n",
      " [1.         0.66666667 0.33333333 0.         0.66666667]\n",
      " [1.         1.         0.66666667 0.66666667 0.        ]]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# demo limitations of scipy dissimilarities for categorical data\\ntest_matrix = [[0, 0, 1], [0, 1, 0], [0, 1, 2], [np.nan, 1, 2], [np.nan, np.nan, 2]]\\nprint(\\\"(Normalized) Hamming distance treats NaN the same as any other value\\\")\\nprint(dist.squareform(pdist(test_matrix, dist.hamming)))\";\n",
       "                var nbb_formatted_code = \"# demo limitations of scipy dissimilarities for categorical data\\ntest_matrix = [[0, 0, 1], [0, 1, 0], [0, 1, 2], [np.nan, 1, 2], [np.nan, np.nan, 2]]\\nprint(\\\"(Normalized) Hamming distance treats NaN the same as any other value\\\")\\nprint(dist.squareform(pdist(test_matrix, dist.hamming)))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# demo limitations of scipy dissimilarities for categorical data\n",
    "test_matrix = [[0, 0, 1], [0, 1, 0], [0, 1, 2],\n",
    "               [np.nan, 1, 2], [np.nan, np.nan, 2]]\n",
    "print('(Normalized) Hamming distance treats NaN the same as any other value')\n",
    "print(dist.squareform(pdist(test_matrix, dist.hamming)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we're comparing points in a space that follows the idealized assumptions from the original applications for Hamming distance:\n",
    "\n",
    "- There are no missing values\n",
    "- All values are binary\n",
    "- All values are balanced, i.e., $p_0 = p_1 = \\frac{1}{2}$\n",
    "- All values are (statistically) independent\n",
    "\n",
    "In this setting, Hamming distance has the effect of measuring the size of the smallest set containing both points from the collection of sets determined by filtering on the coordinate values of points: If two points are binary keys indexing a data set, Hamming distance is the (base-2) logarithm of the number of keys with values matching either point (except when the two points are identical). For every value that is equal between two points, the size of the smallest set containing them both shrinks by half (1 bit). Framed this way, SciPy's practice of incrementing (non-normalized) Hamming distance by 1 for every NaN coordinate is reasonable. But I could just as easily frame it the other direction: for every value that is not equal between two points, the size of the smallest set containing both doubles. Then it isn't so clear that SciPy's Hamming distance is doing the right thing.\n",
    "\n",
    "Alternately, I could simply exclude any variables missing values from the comparison: missing values cut the space under consideration in half rather than doubling the containing set. This could have the side effect of exaggerating the similarity between points with few non-missing values in common. Points with many values in common have more opportunities to disagree, and therefore to be further away just because they aren't missing values. But at least this variation is consistent with both increasing _and_ decreasing the size of the smallest set containing both points—missing values do neither."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42857142857142855\n",
      "0.2\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"a = np.array([0, 0, 0, 1, 1, 1, 0])\\nb = np.array([0, 0, 1, 0, 1, 1, 1])\\nhamming_ignore = ent.hamming_variation_func(\\n    \\\"uniform\\\", \\\"binary\\\", \\\"ignore\\\", np.array([a, b])\\n)\\nprint(hamming_ignore(a, b))\\n\\nc = np.array([0, 0, 0, np.nan, 1, 1, 0])\\nd = np.array([0, 0, np.nan, 0, 1, 1, 1])\\nprint(hamming_ignore(c, d))\";\n",
       "                var nbb_formatted_code = \"a = np.array([0, 0, 0, 1, 1, 1, 0])\\nb = np.array([0, 0, 1, 0, 1, 1, 1])\\nhamming_ignore = ent.hamming_variation_func(\\n    \\\"uniform\\\", \\\"binary\\\", \\\"ignore\\\", np.array([a, b])\\n)\\nprint(hamming_ignore(a, b))\\n\\nc = np.array([0, 0, 0, np.nan, 1, 1, 0])\\nd = np.array([0, 0, np.nan, 0, 1, 1, 1])\\nprint(hamming_ignore(c, d))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.array([0, 0, 0, 1, 1, 1, 0])\n",
    "b = np.array([0, 0, 1, 0, 1, 1, 1])\n",
    "hamming_ignore = ent.hamming_variation_func(\n",
    "    'uniform', 'binary', 'ignore', np.array([a, b])\n",
    ")\n",
    "print(hamming_ignore(a, b))\n",
    "\n",
    "c = np.array([0, 0, 0, np.nan, 1, 1, 0])\n",
    "d = np.array([0, 0, np.nan, 0, 1, 1, 1])\n",
    "print(hamming_ignore(c, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Framing Hamming distance as a measure of the smallest set containing both points inspires a more nuanced measure for when the various assumptions I stated are violated:\n",
    "\n",
    "- When values match, the smallest set containing both points decreases in proportion to that value's probability, which may not be half. So two points are not be particularly close when they have the same value as most of the population, and two points are closer when they share an unusual value.\n",
    "- When values don't match for a non-binary nominal variable, the smallest set containing both is the union of the data with those two values, not the whole population. Two profiles with different minority answers are in a smaller common set than they would be if one of them had a more common answer.\n",
    "- On any variable with an ordinal scale, the smallest common set for two points is the closed interval between them. This provides a basis for extending Hamming distance to handle continuous variables by using differences of quantiles, and a more theory-driven alternative to Gower distance for mixed-type data.\n",
    "\n",
    "This interpretation also allows for an additional nuance: Correlated variables in combination reduce the size of the smallest common set less than uncorrelated variables. However, it could be expensive to calculate the distance between points while accounting for this. The simplest approach would be to filter the data set for values matching either point and count the rows in the result. Left at that, the result would not be a proper metric, since the smallest set containing a point and itself contains at least one point, and thus the \"distance\" would not be 0. That might be corrected by checking for equality of all shared values first and calling the distance 0 if all coordinates match. However, it may be preferable not to. If two identical points are part of a large set of other identical points in a population, we don't necessarily want to consider them close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# Hamming set size calculation prototype (limited to nominal variables)\\ndef get_set_hamming(data):\\n    if type(data) == pd.core.frame.DataFrame:\\n        X = data.values.T\\n    else:\\n        X = np.asanyarray(data).T\\n    n_data = X.shape[1]\\n    n_features = X.shape[0]\\n\\n    @nb.jit\\n    def dist_func(a, b):\\n        match_combined = np.full(n_data, True)\\n        n_non_nan = 0\\n        n_match = 0\\n\\n        for i in range(n_features):\\n            if not np.isnan(a[i]) and not np.isnan(b[i]):\\n                nans = np.isnan(X[i])\\n                match_a = X[i] == a[i]\\n                match_b = X[i] == b[i]\\n                match_either = match_a | match_b | nans\\n                match_combined = match_combined & match_either\\n\\n        sum_combined = np.sum(match_combined)\\n        if sum_combined == 0:\\n            return 1\\n        else:\\n            return np.log2(np.sum(match_combined)) / np.log2(n_data)\\n\\n    return dist_func\";\n",
       "                var nbb_formatted_code = \"# Hamming set size calculation prototype (limited to nominal variables)\\ndef get_set_hamming(data):\\n    if type(data) == pd.core.frame.DataFrame:\\n        X = data.values.T\\n    else:\\n        X = np.asanyarray(data).T\\n    n_data = X.shape[1]\\n    n_features = X.shape[0]\\n\\n    @nb.jit\\n    def dist_func(a, b):\\n        match_combined = np.full(n_data, True)\\n        n_non_nan = 0\\n        n_match = 0\\n\\n        for i in range(n_features):\\n            if not np.isnan(a[i]) and not np.isnan(b[i]):\\n                nans = np.isnan(X[i])\\n                match_a = X[i] == a[i]\\n                match_b = X[i] == b[i]\\n                match_either = match_a | match_b | nans\\n                match_combined = match_combined & match_either\\n\\n        sum_combined = np.sum(match_combined)\\n        if sum_combined == 0:\\n            return 1\\n        else:\\n            return np.log2(np.sum(match_combined)) / np.log2(n_data)\\n\\n    return dist_func\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hamming set size calculation prototype (limited to nominal variables)\n",
    "def get_set_hamming(data):\n",
    "    if type(data) == pd.core.frame.DataFrame:\n",
    "        X = data.values.T\n",
    "    else:\n",
    "        X = np.asanyarray(data).T\n",
    "    n_data = X.shape[1]\n",
    "    n_features = X.shape[0]\n",
    "\n",
    "    @nb.jit\n",
    "    def dist_func(a, b):\n",
    "        match_combined = np.full(n_data, True)\n",
    "        n_non_nan = 0\n",
    "        n_match = 0\n",
    "\n",
    "        for i in range(n_features):\n",
    "            if not np.isnan(a[i]) and not np.isnan(b[i]):\n",
    "                nans = np.isnan(X[i])\n",
    "                match_a = X[i] == a[i]\n",
    "                match_b = X[i] == b[i]\n",
    "                match_either = match_a | match_b | nans\n",
    "                match_combined = match_combined & match_either\n",
    "\n",
    "        sum_combined = np.sum(match_combined)\n",
    "        if sum_combined == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return np.log2(np.sum(match_combined)) / np.log2(n_data)\n",
    "\n",
    "    return dist_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the set-measure-based Hamming distances described above increase distances in the presense of missing values. We sometimes want to minimize the influence of missing values. For this case, I created a family of Hamming distances that can be configured to suit various theoretical and practical goals.\n",
    "\n",
    "- **weight**: Pattern of weights to apply to each feature\n",
    "    - *uniform*: All features have equal weight\n",
    "    - *entropy*: Each feature's weight is proportional to it's (missing-value-aware) entropy in reference data\n",
    "    - *normalized entropy*: Each feature's weight is proportional to it's entropy in reference data relative to perfect entropy for the number of unique non-missing values in that feature\n",
    "- **dist**(ance): Calculation of partial distances for non-missing values\n",
    "    - *binary*: Traditional Hamming distance; 0 for matches, 1 for non-matches\n",
    "    - *entropy*: Measures the smallest set containing both values within each feature (missing-value-aware) in reference data\n",
    "- **missing_dist**(ance): Calculation of partial distances for missing values\n",
    "    - *ignore*: Calculate as if the features with missing values do not exist\n",
    "    - *max*: Maximum partial distance (1), similar to SciPy implementation\n",
    "    - *uniform*: Impute what the mean distance for the feature would be if non-missing values were uniformly distributed, i.e., $\\frac{k-1}{k}$ for a feature with $k$ unique categories\n",
    "    - *mean*: Impute what the mean distance actually is between non-missing values in reference data, including distinguishing between cases where both values being compared are missing vs. only one of the two\n",
    "\n",
    "\n",
    "These functions also require a reference data set for configuration, sometimes to learn value distributions, and always to know how many features to anticipate. They also accept a `missing_ind` parameter to specify different codes for missing values.\n",
    "\n",
    "I test every combination of these parameters to find a metric that minimizes correlation with the number of missing values in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"# test correlation with missing values for various hamming distance configurations\\nn_pairs = 100000  # approximate\\ntest_sample = df.sample(utl.ifloor(np.sqrt(n_pairs * 2))).values\\nn_missing_values = pdist(\\n    test_sample, lambda a, b: np.sum(missing_mask(a) | missing_mask(b))\\n)\\n\\nhamming_param_values = {\\n    \\\"weight\\\": [\\\"uniform\\\", \\\"entropy\\\", \\\"normalized entropy\\\"],\\n    \\\"dist\\\": [\\\"binary\\\", \\\"entropy\\\"],\\n    \\\"missing_dist\\\": [\\\"ignore\\\", \\\"max\\\", \\\"uniform\\\", \\\"mean\\\"],\\n    \\\"data\\\": [test_sample],\\n    \\\"missing_ind\\\": [MISSING_IND],\\n}\\n\\nweight_list = []\\ndist_list = []\\nmissing_list = []\\ntau_list = []\\n\\nfor params in ps.param_grid(hamming_param_values):\\n    hamming_dist = ent.hamming_variation_func(**params)\\n    dists = pdist(test_sample, hamming_dist)\\n    tau, p = st.kendalltau(dists, n_missing_values)\\n\\n    weight_list.append(params[\\\"weight\\\"])\\n    dist_list.append(params[\\\"dist\\\"])\\n    missing_list.append(params[\\\"missing_dist\\\"])\\n    tau_list.append(tau)\\n\\nmissing_correlation_df = pd.DataFrame(\\n    {\\n        \\\"weight\\\": weight_list,\\n        \\\"dist\\\": dist_list,\\n        \\\"missing_dist\\\": missing_list,\\n        \\\"tau\\\": tau_list,\\n        \\\"abs_tau\\\": np.abs(tau_list),\\n    }\\n)\";\n",
       "                var nbb_formatted_code = \"# test correlation with missing values for various hamming distance configurations\\nn_pairs = 100000  # approximate\\ntest_sample = df.sample(utl.ifloor(np.sqrt(n_pairs * 2))).values\\nn_missing_values = pdist(\\n    test_sample, lambda a, b: np.sum(missing_mask(a) | missing_mask(b))\\n)\\n\\nhamming_param_values = {\\n    \\\"weight\\\": [\\\"uniform\\\", \\\"entropy\\\", \\\"normalized entropy\\\"],\\n    \\\"dist\\\": [\\\"binary\\\", \\\"entropy\\\"],\\n    \\\"missing_dist\\\": [\\\"ignore\\\", \\\"max\\\", \\\"uniform\\\", \\\"mean\\\"],\\n    \\\"data\\\": [test_sample],\\n    \\\"missing_ind\\\": [MISSING_IND],\\n}\\n\\nweight_list = []\\ndist_list = []\\nmissing_list = []\\ntau_list = []\\n\\nfor params in ps.param_grid(hamming_param_values):\\n    hamming_dist = ent.hamming_variation_func(**params)\\n    dists = pdist(test_sample, hamming_dist)\\n    tau, p = st.kendalltau(dists, n_missing_values)\\n\\n    weight_list.append(params[\\\"weight\\\"])\\n    dist_list.append(params[\\\"dist\\\"])\\n    missing_list.append(params[\\\"missing_dist\\\"])\\n    tau_list.append(tau)\\n\\nmissing_correlation_df = pd.DataFrame(\\n    {\\n        \\\"weight\\\": weight_list,\\n        \\\"dist\\\": dist_list,\\n        \\\"missing_dist\\\": missing_list,\\n        \\\"tau\\\": tau_list,\\n        \\\"abs_tau\\\": np.abs(tau_list),\\n    }\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test correlation with missing values for various hamming distance configurations\n",
    "n_pairs = 100000  # approximate\n",
    "test_sample = df.sample(utl.ifloor(np.sqrt(n_pairs * 2))).values\n",
    "n_missing_values = pdist(\n",
    "    test_sample, lambda a, b: np.sum(missing_mask(a) | missing_mask(b))\n",
    ")\n",
    "\n",
    "hamming_param_values = {\n",
    "    'weight': ['uniform', 'entropy', 'normalized entropy'],\n",
    "    'dist': ['binary', 'entropy'],\n",
    "    'missing_dist': ['ignore', 'max', 'uniform', 'mean'],\n",
    "    'data': [test_sample],\n",
    "    'missing_ind': [MISSING_IND],\n",
    "}\n",
    "\n",
    "weight_list = []\n",
    "dist_list = []\n",
    "missing_list = []\n",
    "tau_list = []\n",
    "\n",
    "for params in ps.param_grid(hamming_param_values):\n",
    "    hamming_dist = ent.hamming_variation_func(**params)\n",
    "    dists = pdist(test_sample, hamming_dist)\n",
    "    tau, p = st.kendalltau(dists, n_missing_values)\n",
    "\n",
    "    weight_list.append(params['weight'])\n",
    "    dist_list.append(params['dist'])\n",
    "    missing_list.append(params['missing_dist'])\n",
    "    tau_list.append(tau)\n",
    "\n",
    "missing_correlation_df = pd.DataFrame(\n",
    "    {\n",
    "        'weight': weight_list,\n",
    "        'dist': dist_list,\n",
    "        'missing_dist': missing_list,\n",
    "        'tau': tau_list,\n",
    "        'abs_tau': np.abs(tau_list),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>dist</th>\n",
       "      <th>missing_dist</th>\n",
       "      <th>tau</th>\n",
       "      <th>abs_tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>entropy</td>\n",
       "      <td>binary</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.001848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>normalized entropy</td>\n",
       "      <td>binary</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.017754</td>\n",
       "      <td>0.017754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uniform</td>\n",
       "      <td>binary</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>0.041315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>entropy</td>\n",
       "      <td>binary</td>\n",
       "      <td>ignore</td>\n",
       "      <td>-0.168310</td>\n",
       "      <td>0.168310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>normalized entropy</td>\n",
       "      <td>binary</td>\n",
       "      <td>ignore</td>\n",
       "      <td>-0.215509</td>\n",
       "      <td>0.215509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>normalized entropy</td>\n",
       "      <td>binary</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.608756</td>\n",
       "      <td>0.608756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>entropy</td>\n",
       "      <td>binary</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.616932</td>\n",
       "      <td>0.616932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uniform</td>\n",
       "      <td>binary</td>\n",
       "      <td>ignore</td>\n",
       "      <td>-0.628561</td>\n",
       "      <td>0.628561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uniform</td>\n",
       "      <td>binary</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.724781</td>\n",
       "      <td>0.724781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uniform</td>\n",
       "      <td>entropy</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.781830</td>\n",
       "      <td>0.781830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>normalized entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.866799</td>\n",
       "      <td>0.866799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.868854</td>\n",
       "      <td>0.868854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>entropy</td>\n",
       "      <td>binary</td>\n",
       "      <td>max</td>\n",
       "      <td>0.873412</td>\n",
       "      <td>0.873412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>normalized entropy</td>\n",
       "      <td>binary</td>\n",
       "      <td>max</td>\n",
       "      <td>0.880666</td>\n",
       "      <td>0.880666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uniform</td>\n",
       "      <td>binary</td>\n",
       "      <td>max</td>\n",
       "      <td>0.907791</td>\n",
       "      <td>0.907791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.939054</td>\n",
       "      <td>0.939054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>normalized entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>max</td>\n",
       "      <td>0.947883</td>\n",
       "      <td>0.947883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>max</td>\n",
       "      <td>0.948334</td>\n",
       "      <td>0.948334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>normalized entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.950780</td>\n",
       "      <td>0.950780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>uniform</td>\n",
       "      <td>entropy</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.978925</td>\n",
       "      <td>0.978925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>ignore</td>\n",
       "      <td>0.984503</td>\n",
       "      <td>0.984503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>normalized entropy</td>\n",
       "      <td>entropy</td>\n",
       "      <td>ignore</td>\n",
       "      <td>0.988517</td>\n",
       "      <td>0.988517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uniform</td>\n",
       "      <td>entropy</td>\n",
       "      <td>ignore</td>\n",
       "      <td>0.988804</td>\n",
       "      <td>0.988804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>uniform</td>\n",
       "      <td>entropy</td>\n",
       "      <td>max</td>\n",
       "      <td>0.997490</td>\n",
       "      <td>0.997490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                weight     dist missing_dist       tau   abs_tau\n",
       "11             entropy   binary         mean  0.001848  0.001848\n",
       "19  normalized entropy   binary         mean  0.017754  0.017754\n",
       "3              uniform   binary         mean  0.041315  0.041315\n",
       "8              entropy   binary       ignore -0.168310  0.168310\n",
       "16  normalized entropy   binary       ignore -0.215509  0.215509\n",
       "18  normalized entropy   binary      uniform  0.608756  0.608756\n",
       "10             entropy   binary      uniform  0.616932  0.616932\n",
       "0              uniform   binary       ignore -0.628561  0.628561\n",
       "2              uniform   binary      uniform  0.724781  0.724781\n",
       "7              uniform  entropy         mean  0.781830  0.781830\n",
       "23  normalized entropy  entropy         mean  0.866799  0.866799\n",
       "15             entropy  entropy         mean  0.868854  0.868854\n",
       "9              entropy   binary          max  0.873412  0.873412\n",
       "17  normalized entropy   binary          max  0.880666  0.880666\n",
       "1              uniform   binary          max  0.907791  0.907791\n",
       "14             entropy  entropy      uniform  0.939054  0.939054\n",
       "21  normalized entropy  entropy          max  0.947883  0.947883\n",
       "13             entropy  entropy          max  0.948334  0.948334\n",
       "22  normalized entropy  entropy      uniform  0.950780  0.950780\n",
       "6              uniform  entropy      uniform  0.978925  0.978925\n",
       "12             entropy  entropy       ignore  0.984503  0.984503\n",
       "20  normalized entropy  entropy       ignore  0.988517  0.988517\n",
       "4              uniform  entropy       ignore  0.988804  0.988804\n",
       "5              uniform  entropy          max  0.997490  0.997490"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"missing_correlation_df.sort_values(\\\"abs_tau\\\")\";\n",
       "                var nbb_formatted_code = \"missing_correlation_df.sort_values(\\\"abs_tau\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "missing_correlation_df.sort_values('abs_tau')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configurations with `dist=\"binary\"` consistently correlate least with missing values. `dist=\"entropy\"` metrics include distributions of missing values in the calculation, so it's no surprise that it has a strong relationship with missing values—after all, that's exactly what I created the missing-value-aware entropy for, in the context of feature comparisons.\n",
    "\n",
    "It's less clear why configurations using `missing_dist=\"mean\"` should be less correlated with missing values than other options—especially `ignore`. By ignoring missing values entirely, the `ignore` metrics proportionally increase the influence of the remaining non-missing values, creating substantial negative correlations. Dropping rows with missing values from a data set can bias an analysis, and dropping columns is no different. The `mean` and `uniform` options both impute partial distances for missing values, and the approach used by `mean` is more context-aware than `uniform`—like the difference betwen interpolation and mean imputation.\n",
    "\n",
    "`weight=\"entropy\"` metrics correlate less with missing values compared to similar metrics using either `normalized entropy` or `uniform`. `entropy` and `normalized entropy` both down-weight features with more missing values (in addition to features with imbalanced classes), and `normalized entropy` compresses the range of weights compared to `entropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"# set measure Hamming distances on full data set & sample of data\\nhamming_set = get_set_hamming(df)\\nhamming_set_sample = get_set_hamming(df.sample(5000))\\n\\n# set measure Hamming distance assuming variables are independently distributed\\nhamming_set_ind = ent.hamming_variation_func(\\n    \\\"uniform\\\", \\\"entropy\\\", \\\"max\\\", df, MISSING_IND\\n)\\n\\n# Hamming distance chosen to correlate least with missing values\\nhamming_decorr = ent.hamming_variation_func(\\n    \\\"entropy\\\", \\\"binary\\\", \\\"mean\\\", df, MISSING_IND\\n)\";\n",
       "                var nbb_formatted_code = \"# set measure Hamming distances on full data set & sample of data\\nhamming_set = get_set_hamming(df)\\nhamming_set_sample = get_set_hamming(df.sample(5000))\\n\\n# set measure Hamming distance assuming variables are independently distributed\\nhamming_set_ind = ent.hamming_variation_func(\\n    \\\"uniform\\\", \\\"entropy\\\", \\\"max\\\", df, MISSING_IND\\n)\\n\\n# Hamming distance chosen to correlate least with missing values\\nhamming_decorr = ent.hamming_variation_func(\\n    \\\"entropy\\\", \\\"binary\\\", \\\"mean\\\", df, MISSING_IND\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set measure Hamming distances on full data set & sample of data\n",
    "hamming_set = get_set_hamming(df)\n",
    "hamming_set_sample = get_set_hamming(df.sample(5000))\n",
    "\n",
    "# set measure Hamming distance assuming variables are independently distributed\n",
    "hamming_set_ind = ent.hamming_variation_func(\n",
    "    'uniform', 'entropy', 'max', df, MISSING_IND\n",
    ")\n",
    "\n",
    "# Hamming distance chosen to correlate least with missing values\n",
    "hamming_decorr = ent.hamming_variation_func(\n",
    "    'entropy', 'binary', 'mean', df, MISSING_IND\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"# sample hamming distances\\nn_pairs = len(df)\\ntest_sample = df.sample(utl.ifloor(np.sqrt(n_pairs))).values\\n# n_missing_values = [np.sum(missing_mask(a) | missing_mask(b)) for a, b in itr.combinations(test_sample)]\\n\\nn_missing_values = pdist(\\n    test_sample, lambda a, b: np.sum(missing_mask(a) | missing_mask(b))\\n)\\ndata = {\\n    \\\"scipy\\\": pdist(test_sample, dist.hamming),\\n    \\\"set\\\": pdist(test_sample, hamming_set),\\n    \\\"set_sample\\\": pdist(test_sample, hamming_set_sample),\\n    \\\"set_ind\\\": pdist(test_sample, hamming_set_ind),\\n    \\\"decorr\\\": pdist(test_sample, hamming_decorr),\\n}\\nhamming_df = pd.DataFrame(data)\";\n",
       "                var nbb_formatted_code = \"# sample hamming distances\\nn_pairs = len(df)\\ntest_sample = df.sample(utl.ifloor(np.sqrt(n_pairs))).values\\n# n_missing_values = [np.sum(missing_mask(a) | missing_mask(b)) for a, b in itr.combinations(test_sample)]\\n\\nn_missing_values = pdist(\\n    test_sample, lambda a, b: np.sum(missing_mask(a) | missing_mask(b))\\n)\\ndata = {\\n    \\\"scipy\\\": pdist(test_sample, dist.hamming),\\n    \\\"set\\\": pdist(test_sample, hamming_set),\\n    \\\"set_sample\\\": pdist(test_sample, hamming_set_sample),\\n    \\\"set_ind\\\": pdist(test_sample, hamming_set_ind),\\n    \\\"decorr\\\": pdist(test_sample, hamming_decorr),\\n}\\nhamming_df = pd.DataFrame(data)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample hamming distances\n",
    "n_pairs = len(df)\n",
    "test_sample = df.sample(utl.ifloor(np.sqrt(n_pairs))).values\n",
    "\n",
    "n_missing_values = pdist(\n",
    "    test_sample, lambda a, b: np.sum(missing_mask(a) | missing_mask(b))\n",
    ")\n",
    "data = {\n",
    "    'scipy': pdist(test_sample, dist.hamming),\n",
    "    'set': pdist(test_sample, hamming_set),\n",
    "    'set_sample': pdist(test_sample, hamming_set_sample),\n",
    "    'set_ind': pdist(test_sample, hamming_set_ind),\n",
    "    'decorr': pdist(test_sample, hamming_decorr),\n",
    "}\n",
    "hamming_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scipy</th>\n",
       "      <th>set</th>\n",
       "      <th>set_sample</th>\n",
       "      <th>set_ind</th>\n",
       "      <th>decorr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>33930.000000</td>\n",
       "      <td>33930.000000</td>\n",
       "      <td>33930.000000</td>\n",
       "      <td>33930.000000</td>\n",
       "      <td>33930.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.937432</td>\n",
       "      <td>0.454855</td>\n",
       "      <td>0.587436</td>\n",
       "      <td>0.894585</td>\n",
       "      <td>0.523454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.067802</td>\n",
       "      <td>0.266864</td>\n",
       "      <td>0.289103</td>\n",
       "      <td>0.126908</td>\n",
       "      <td>0.020664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.522858</td>\n",
       "      <td>0.062262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110570</td>\n",
       "      <td>0.431033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.930081</td>\n",
       "      <td>0.062262</td>\n",
       "      <td>0.345705</td>\n",
       "      <td>0.878567</td>\n",
       "      <td>0.512890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.969650</td>\n",
       "      <td>0.510835</td>\n",
       "      <td>0.575926</td>\n",
       "      <td>0.955804</td>\n",
       "      <td>0.520908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.978871</td>\n",
       "      <td>0.675265</td>\n",
       "      <td>0.807218</td>\n",
       "      <td>0.971945</td>\n",
       "      <td>0.531956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.998847</td>\n",
       "      <td>0.939799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998456</td>\n",
       "      <td>0.676982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              scipy           set    set_sample       set_ind        decorr\n",
       "count  33930.000000  33930.000000  33930.000000  33930.000000  33930.000000\n",
       "mean       0.937432      0.454855      0.587436      0.894585      0.523454\n",
       "std        0.067802      0.266864      0.289103      0.126908      0.020664\n",
       "min        0.522858      0.062262      0.000000      0.110570      0.431033\n",
       "25%        0.930081      0.062262      0.345705      0.878567      0.512890\n",
       "50%        0.969650      0.510835      0.575926      0.955804      0.520908\n",
       "75%        0.978871      0.675265      0.807218      0.971945      0.531956\n",
       "max        0.998847      0.939799      1.000000      0.998456      0.676982"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"hamming_df.describe()\";\n",
       "                var nbb_formatted_code = \"hamming_df.describe()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hamming_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scipy</th>\n",
       "      <th>set</th>\n",
       "      <th>set_sample</th>\n",
       "      <th>set_ind</th>\n",
       "      <th>decorr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>scipy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.776294</td>\n",
       "      <td>-0.442256</td>\n",
       "      <td>0.991072</td>\n",
       "      <td>0.014585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set</th>\n",
       "      <td>0.776294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.314022</td>\n",
       "      <td>0.774750</td>\n",
       "      <td>0.005069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set_sample</th>\n",
       "      <td>-0.442256</td>\n",
       "      <td>-0.314022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.452729</td>\n",
       "      <td>0.101978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>set_ind</th>\n",
       "      <td>0.991072</td>\n",
       "      <td>0.774750</td>\n",
       "      <td>-0.452729</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.088539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decorr</th>\n",
       "      <td>0.014585</td>\n",
       "      <td>0.005069</td>\n",
       "      <td>0.101978</td>\n",
       "      <td>-0.088539</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               scipy       set  set_sample   set_ind    decorr\n",
       "scipy       1.000000  0.776294   -0.442256  0.991072  0.014585\n",
       "set         0.776294  1.000000   -0.314022  0.774750  0.005069\n",
       "set_sample -0.442256 -0.314022    1.000000 -0.452729  0.101978\n",
       "set_ind     0.991072  0.774750   -0.452729  1.000000 -0.088539\n",
       "decorr      0.014585  0.005069    0.101978 -0.088539  1.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"hamming_df.corr()\";\n",
       "                var nbb_formatted_code = \"hamming_df.corr()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hamming_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"subsample = df.sample(min(len(df), 1000)).values\";\n",
       "                var nbb_formatted_code = \"subsample = df.sample(min(len(df), 1000)).values\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subsample = df.sample(min(len(df), 1000)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 0:00:06.570011\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"scipy_time = utl.runtimer()\\ndistances = pdist(subsample, dist.hamming)\\nscipy_time()\";\n",
       "                var nbb_formatted_code = \"scipy_time = utl.runtimer()\\ndistances = pdist(subsample, dist.hamming)\\nscipy_time()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scipy_time = utl.runtimer()\n",
    "distances = pdist(subsample, dist.hamming)\n",
    "scipy_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 0:02:53.721412\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"sample_time = utl.runtimer()\\ndistances = pdist(subsample, hamming_set_sample)\\nsample_time()\";\n",
       "                var nbb_formatted_code = \"sample_time = utl.runtimer()\\ndistances = pdist(subsample, hamming_set_sample)\\nsample_time()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_time = utl.runtimer()\n",
    "distances = pdist(subsample, hamming_set_sample)\n",
    "sample_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 0:00:08.696891\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"ind_time = utl.runtimer()\\ndistances = pdist(subsample, hamming_set_ind)\\nind_time()\";\n",
       "                var nbb_formatted_code = \"ind_time = utl.runtimer()\\ndistances = pdist(subsample, hamming_set_ind)\\nind_time()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind_time = utl.runtimer()\n",
    "distances = pdist(subsample, hamming_set_ind)\n",
    "ind_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 0:00:13.451311\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"decorr_time = utl.runtimer()\\ndistances = pdist(subsample, hamming_decorr)\\ndecorr_time()\";\n",
       "                var nbb_formatted_code = \"decorr_time = utl.runtimer()\\ndistances = pdist(subsample, hamming_decorr)\\ndecorr_time()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decorr_time = utl.runtimer()\n",
    "distances = pdist(subsample, hamming_decorr)\n",
    "decorr_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when using a small subsample of the reference data, the variation directly using set size is abysmally slow (not to mention the negative relationship of the sample-based set measure distance with it's population counterpart). A suitable data structure might accelerate the computation, such as a materialized OLAP cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": "16",
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
